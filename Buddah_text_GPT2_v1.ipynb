{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+ascwMYTE6Zj5CR89NRnb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fwVUDoSYF0e",
        "outputId": "2caca622-89f5-457d-f4c8-3cb31028738e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the CSV data\n",
        "data = pd.read_csv(\"buddhist_q_a.csv\")\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_qa(q, a):\n",
        "    return f\"Question: {q}\\nAnswer: {a}\\n\"\n",
        "\n",
        "# Combine the questions and answers using the preprocess function\n",
        "text_data = [preprocess_qa(row[\"question\"], row[\"answer\"]) for _, row in data.iterrows()]\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_data = [tokenizer.encode(text) for text in text_data]"
      ],
      "metadata": {
        "id": "OX0yUXX2Ybvy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer, AdamW\n",
        "\n",
        "# Custom Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_texts):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokenized_texts[idx]\n",
        "\n",
        "# Custom Collator class\n",
        "class Collator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        inputs = [torch.tensor(item, dtype=torch.long) for item in batch]\n",
        "        inputs = pad_sequence(inputs, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        return inputs\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 4\n",
        "epochs = 6\n",
        "lr = 1e-5\n",
        "\n",
        "# Load the pre-trained GPT-2 model\n",
        "config = GPT2Config.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "\n",
        "# Create the Dataset and DataLoader\n",
        "dataset = TextDataset(tokenized_data)\n",
        "collator_fn = Collator(tokenizer)\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator_fn)\n",
        "\n",
        "# Move the model to the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_dataloader:\n",
        "        # Move the batch to the device\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=batch, labels=batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print the loss\n",
        "        print(f\"Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "reAJrjYkYcCb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8fb818e-2de4-4436-bc95-feb8b547f34e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.492715835571289\n",
            "Loss: 6.328117847442627\n",
            "Loss: 5.293035984039307\n",
            "Loss: 4.846797466278076\n",
            "Loss: 4.42454719543457\n",
            "Loss: 3.9342041015625\n",
            "Loss: 3.9833872318267822\n",
            "Loss: 3.821706771850586\n",
            "Loss: 3.3678717613220215\n",
            "Loss: 3.158482313156128\n",
            "Loss: 2.8140087127685547\n",
            "Loss: 2.785637378692627\n",
            "Loss: 2.922175168991089\n",
            "Loss: 2.276271104812622\n",
            "Loss: 2.1758272647857666\n",
            "Loss: 2.66987943649292\n",
            "Loss: 2.355759620666504\n",
            "Loss: 2.5381062030792236\n",
            "Loss: 2.6413075923919678\n",
            "Loss: 2.678985834121704\n",
            "Loss: 2.123674154281616\n",
            "Loss: 2.1510376930236816\n",
            "Loss: 2.3437981605529785\n",
            "Loss: 2.2679924964904785\n",
            "Loss: 2.4316518306732178\n",
            "Loss: 2.6425180435180664\n",
            "Loss: 1.8036822080612183\n",
            "Loss: 1.3015156984329224\n",
            "Loss: 2.0854525566101074\n",
            "Loss: 1.9218826293945312\n",
            "Loss: 2.1241402626037598\n",
            "Loss: 2.2575793266296387\n",
            "Loss: 2.3994438648223877\n",
            "Loss: 1.9485573768615723\n",
            "Loss: 2.0097875595092773\n",
            "Loss: 2.1713318824768066\n",
            "Loss: 2.0759596824645996\n",
            "Loss: 2.159410238265991\n",
            "Loss: 2.4755184650421143\n",
            "Loss: 1.5593750476837158\n",
            "Loss: 1.2407606840133667\n",
            "Loss: 1.7791895866394043\n",
            "Loss: 1.7034201622009277\n",
            "Loss: 1.990344524383545\n",
            "Loss: 2.0558104515075684\n",
            "Loss: 2.189720392227173\n",
            "Loss: 1.7735497951507568\n",
            "Loss: 1.8957383632659912\n",
            "Loss: 2.0190863609313965\n",
            "Loss: 1.8848443031311035\n",
            "Loss: 2.0944464206695557\n",
            "Loss: 2.2554566860198975\n",
            "Loss: 1.4427220821380615\n",
            "Loss: 1.1015794277191162\n",
            "Loss: 1.582837700843811\n",
            "Loss: 1.5881954431533813\n",
            "Loss: 1.8714429140090942\n",
            "Loss: 1.982377529144287\n",
            "Loss: 2.1106131076812744\n",
            "Loss: 1.6693905591964722\n",
            "Loss: 1.7836607694625854\n",
            "Loss: 1.9040473699569702\n",
            "Loss: 1.7734456062316895\n",
            "Loss: 2.0593528747558594\n",
            "Loss: 1.98441481590271\n",
            "Loss: 1.364700436592102\n",
            "Loss: 1.0781874656677246\n",
            "Loss: 1.598221778869629\n",
            "Loss: 1.5169740915298462\n",
            "Loss: 1.7686822414398193\n",
            "Loss: 1.8660861253738403\n",
            "Loss: 2.0379090309143066\n",
            "Loss: 1.5592210292816162\n",
            "Loss: 1.7344143390655518\n",
            "Loss: 1.8429510593414307\n",
            "Loss: 1.6554197072982788\n",
            "Loss: 1.855032205581665\n",
            "Loss: 1.9267754554748535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FilFLfn5Z43n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"fine_tuned_gpt2_buddhism_ai\")"
      ],
      "metadata": {
        "id": "Ua6zCAWgZ5Bp"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}